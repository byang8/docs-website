---
title: Kafka monitoring integration
tags:
  - Integrations
  - On-host integrations
  - On-host integrations list
metaDescription: "New Relic's Kafka integration: how to install it and configure it, and what data it reports."
redirects:
  - /docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration
  - /docs/kafka-integration-new-relic-infrastructure
---

The New Relic Kafka [on-host integration](/docs/integrations/host-integrations/getting-started/introduction-host-integrations) reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics.

Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see [Instrument Kafka message queues](/docs/agents/java-agent/instrumentation/instrument-kafka-message-queues).

## Compatibility and requirements [#req]

Our integration is compatible with Kafka versions 3.0 or below.

Before installing the integration, make sure that you meet the following requirements:

- A New Relic account. Don't have one? [Sign up for free!](https://newrelic.com/signup) No credit card required.
- If Kafka is **not** running on Kubernetes or Amazon ECS, you must [install the infrastructure agent](/docs/infrastructure/install-infrastructure-agent/get-started/install-infrastructure-agent-new-relic) on a host that's running Kafka. Otherwise:
  - If running on Kubernetes, see [these requirements](/docs/monitor-service-running-kubernetes#requirements).
  - If running on ECS, see [these requirements](/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs).
- Java 8 or higher
- JMX enabled on all brokers
- Java-based consumers and producers only, and with JMX enabled
- Total number of monitored topics must be fewer than 10000

For Kafka running on Kubernetes, see [the Kubernetes requirements](http://docs.newrelic.com/docs/monitor-service-running-kubernetes#requirements).

## Prepare for the installation [#prepare]

Kafka is a complex piece of software that is built as a distributed system. For this reason, you’ll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly.

### Autodiscovery [#autodiscovery]

Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored.

#### Bootstrap [#bootstrap]

With the [bootstrap mechanism](#bootstrap-discovery), the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work.

#### Zookeeper [#zookeeper]

Alternatively, the Kafka integration can also talk to a [Zookeeper server](#zookeeper-discovery) in order to obtain the list of brokers. To do this, the integration needs to be provided with the following:

- The list of Zookeeper hosts to contact (zookeeper_hosts).
- The proper authentication secrets to connect with the hosts.

Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker.

You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds.

<Callout variant="tip">

The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it.

</Callout>

### Topic listing [#topic-listing]

To correctly list the topics processed by the brokers, the integration needs to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. The topics must have DESCRIBE access.

### Broker monitoring (JMX) [#broker-monitoring]

The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX.

You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly.

If autodiscovery is set to bootstrap, the JMX settings defined for the bootstrap broker will be applied for all other discovered brokers, so the Port and other settings should be the same on all the brokers.

<Callout variant="important">

We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk.

</Callout>

### Consumer offset [#consumer-offset]

The offset of the consumer and consumer groups of the topics as well as the lag, can be retrieved as a [KafkaOffsetSample](#KafkaOffsetSample-collection) with the `CONSUMER_OFFSET=true` flag but should be in a separate instance because when this flag is activated the instance will not collect other Samples.

### Producer/consumer monitoring (JMX) [#producer]

Producers and consumers written in Java can also be monitored to get more specific metadata through the same mechanism (JMX). This will generate [KafkaConsumerSamples and KafkaProducerSamples](#KafkaConsumerSample-collection). JMX needs to be enabled and configured on those applications where it is not enabled by default.

Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration.

### Connectivity requirements [#connectivity-requirements]

As a summary, the integration needs to be configured and allowed to connect to:

- Hosts listed in `zookeeper_hosts` over the Zookeeper protocol, using the Zookeeper authentication mechanism (if `autodiscover_strategy` is set to `zookeeper`).
- Hosts defined in `bootstrap_broker_host` over the Kafka protocol, using the Kafka broker’s authentication/transport mechanisms (if `autodiscover_strategy` is set to `bootstrap`).
- All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms.
- All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers.
- All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers.

<Callout variant="important">

**For the cloud:** By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and **must** be open for the integration to be able to connect to and collect metrics from brokers.

</Callout>

## Install and activate [#install]

To install the Kafka integration, choose your setup:

<CollapserGroup>
  <Collapser
    id="ecs-install"
    title="ECS"
  >
    See [Monitor service running on ECS](/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs).
  </Collapser>

{' '}

<Collapser id="k8s-install" title="Kubernetes">
  See [Monitor service running on
  Kubernetes](/docs/monitor-service-running-kubernetes).
</Collapser>

  <Collapser
    id="linux-install"
    title="Linux installation"
  >
    1. Follow the instructions for [installing an integration](/docs/install-integrations-package), using the file name `nri-kafka`.
    2. Change the directory to the integrations configuration folder:

       ```
       cd /etc/newrelic-infra/integrations.d
       ```
    3. Copy of the sample configuration file:

       ```
       sudo cp kafka-config.yml.sample kafka-config.yml
       ```
    4. Edit the `kafka-config.yml` file as described in the [configuration settings](#config).
    5. [Restart](/docs/infrastructure/new-relic-infrastructure/configuration/start-stop-restart-check-infrastructure-agent-status) the Infrastructure agent.

  </Collapser>

  <Collapser
    id="windows-install"
    title="Windows installation"
  >
    1. Download the `nri-kafka` installer image from:

       [http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe](http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64.msi)
    2. To install from the Windows command prompt, run:

       ```
       <var>PATH\TO\</var>nri-kafka-amd64-installer.exe
       ```
    3. In the Integrations directory, `C:\Program Files\New Relic\newrelic-infra\integrations.d\`, create a copy of the sample configuration file by running:

       ```
       cp kafka-config.yml.sample kafka-config.yml
       ```
    4. Edit the `kafka-config.yml` configuration as described in the [configuration settings](#config).
    5. [Restart the infrastructure agent](/docs/infrastructure/new-relic-infrastructure/configuration/start-stop-restart-check-infrastructure-agent-status).

  </Collapser>
</CollapserGroup>

Additional notes:

- **Advanced:** It's also possible to [install the integration from a tarball file](/docs/integrations/host-integrations/installation/install-host-integrations-built-new-relic#tarball). This gives you full control over the installation and configuration process.
- **On-host integrations do not automatically update.** For best results, regularly [update the integration package](/docs/integrations/host-integrations/installation/update-infrastructure-host-integration-package) and [the infrastructure agent](/docs/infrastructure/new-relic-infrastructure/installation/update-infrastructure-agent).

## Configure the integration [#config]

There are several ways to configure the integration, depending on how it was installed:

- If enabled via Kubernetes: see [Monitor services running on Kubernetes](/docs/monitor-service-running-kubernetes).
- If enabled via Amazon ECS: see [Monitor services running on ECS](/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs).
- If installed on-host: edit the config in the integration's YAML config file, `kafka-config.yml`.

An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment.

The configuration file has common settings applicable to all integrations like `interval`, `timeout`, `inventory_source`. To read all about these common settings refer to our [Configuration Format](/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integrations-newer-configuration-format/#configuration-basics) document.

<Callout variant="important">
  If you are still using our Legacy configuration/definition files please refer
  to this
  [document](/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integrations-standard-configuration-format/)
  for help.
</Callout>

Specific settings related to Kafka are defined using the `env` section of the configuration file. These settings control the connection to your Brokers, Zookeeper and JMX as well as other security settings and features. The list of valid settings is described in the next section of this document.

<Callout variant="important">
  The integration has two modes of operation, which are mutually exclusive: “Core” collection and "Consumer offset collection" controlled by the CONSUMER_OFFSET parameter:
  - `CONSUMER_OFFSET = true` is consumer offset collection mode, and will produce [KafkaOffsetSample](#KafkaOffsetSample-collection).
  - `CONSUMER_OFFSET = false` is core collection mode, which can collect the rest of the samples ([KafkaBrokerSample, KafkaTopicSample](#broker-collection), [KafkaProducerSample, KafkaConsumerSample](#KafkaConsumerSample-collection)).

These modes are separated because consumer offset collection takes a long time to run and has high performance requirements.

</Callout>

The values for these settings can be defined in several ways:

- Adding the value directly in the config file. This is the most common way.
- Replacing the values from environment variables using the `{{}}` notation. This requires infrastructure agent v1.14.0+. Read more [here](/docs/infrastructure/install-infrastructure-agent/configuration/configure-infrastructure-agent/#passthrough).
- Using Secrets management. Use this to protect sensible information such as passwords to be exposed in plain text on the configuration file. For more information, see [Secrets management](/docs/integrations/host-integrations/installation/secrets-management).



## Example configurations [#examples]

<CollapserGroup>
  <Collapser
    id="example1"
    title="ZOOKEEPER DISCOVERY"
  >
    This configuration collects Metrics and Inventory including all topics discovering the brokers from two different JMX hosts :

    ```
    integrations:
      - name: nri-kafka
        env:
          CLUSTER_NAME: testcluster1
          KAFKA_VERSION: "1.0.0"
          AUTODISCOVER_STRATEGY: zookeeper
          ZOOKEEPER_HOSTS: '[{"host": "localhost", "port": 2181}, {"host": "localhost2", "port": 2181}]'
          ZOOKEEPER_AUTH_SECRET: "username:password"
          ZOOKEEPER_PATH: "/kafka-root"
          DEFAULT_JMX_USER: username
          DEFAULT_JMX_PASSWORD: password
          TOPIC_MODE: all
        interval: 15s
        labels:
          env: production
          role: kafka
        inventory_source: config/kafka
    ```

  </Collapser>

  <Collapser
    id="example2"
    title="ZOOKEEPER SSL DISCOVERY"
  >
    This configuration collects Metrics and Inventory discovering the brokers from a JMX host with SSL :

    ```
    integrations:
      - name: nri-kafka
        env:
          CLUSTER_NAME: testcluster1
          KAFKA_VERSION: "1.0.0"
          AUTODISCOVER_STRATEGY: zookeeper
          ZOOKEEPER_HOSTS: '[{"host": "localhost", "port": 2181}]'
          ZOOKEEPER_AUTH_SECRET: "username:password"
          ZOOKEEPER_PATH: "/kafka-root"
          DEFAULT_JMX_USER: username
          DEFAULT_JMX_PASSWORD: password

          KEY_STORE: "/path/to/your/keystore"
          KEY_STORE_PASSWORD: keystore_password
          TRUST_STORE: "/path/to/your/truststore"
          TRUST_STORE_PASSWORD: truststore_password

          TIMEOUT: 10000  #The timeout for individual JMX queries in milliseconds.
        interval: 15s
        labels:
          env: production
          role: kafka
        inventory_source: config/kafka
    ```

  </Collapser>
  <Collapser
    id="example3"
    title="BOOOTSTRAP DISCOVERY"
  >
    This configuration collects Metrics and Inventory including all topics discovering the brokers from one bootstrap broker :

    ```
    integrations:
      - name: nri-kafka
        env:
          CLUSTER_NAME: testcluster1
          AUTODISCOVER_STRATEGY: bootstrap
          BOOTSTRAP_BROKER_HOST: localhost
          BOOTSTRAP_BROKER_KAFKA_PORT: 9092
          BOOTSTRAP_BROKER_KAFKA_PROTOCOL: PLAINTEXT
          BOOTSTRAP_BROKER_JMX_PORT: 9999  # This same port will be used to connect to all discover broker JMX
          BOOTSTRAP_BROKER_JMX_USER: admin
          BOOTSTRAP_BROKER_JMX_PASSWORD: password

          LOCAL_ONLY_COLLECTION: false

          COLLECT_BROKER_TOPIC_DATA: true
          TOPIC_MODE: "all"
          COLLECT_TOPIC_SIZE: false
        interval: 15s
        labels:
          env: production
          role: kafka
        inventory_source: config/kafka
    ```

  </Collapser>
  <Collapser
    id="example4"
    title="BOOOTSTRAP DISCOVERY TLS"
  >
    This configuration collects only Metrics discovering the brokers from one bootstrap broker listening with TLS protocol :

    ```
    integrations:
      - name: nri-kafka
        env:
          METRICS: true
          CLUSTER_NAME: testcluster1
          AUTODISCOVER_STRATEGY: bootstrap
          BOOTSTRAP_BROKER_HOST: localhost
          BOOTSTRAP_BROKER_KAFKA_PORT: 9092
          BOOTSTRAP_BROKER_KAFKA_PROTOCOL: SSL
          BOOTSTRAP_BROKER_JMX_PORT: 9999
          BOOTSTRAP_BROKER_JMX_USER: admin
          BOOTSTRAP_BROKER_JMX_PASSWORD: password

          # Kerberos authentication arguments
          TLS_CA_FILE: "/path/to/CA.pem"
          TLS_CERT_FILE: "/path/to/cert.pem"
          TLS_KEY_FILE: "/path/to/key.pem"
          TLS_INSECURE_SKIP_VERIFY: false
        interval: 15s
        labels:
          env: production
          role: kafka
        inventory_source: config/kafka
    ```

  </Collapser>
  <Collapser
    id="example5"
    title="BOOOTSTRAP DISCOVERY KERBEROS AUTH"
  >
    This configuration collects only Metrics discovering the brokers from one bootstrap broker in a Kerberos Auth Cluster :

    ```
    integrations:
      - name: nri-kafka
        env:
          METRICS: true
          CLUSTER_NAME: testcluster1
          AUTODISCOVER_STRATEGY: bootstrap
          BOOTSTRAP_BROKER_HOST: localhost
          BOOTSTRAP_BROKER_KAFKA_PORT: 9092
          BOOTSTRAP_BROKER_KAFKA_PROTOCOL: PLAINTEXT # Currently support PLAINTEXT and SSL
          BOOTSTRAP_BROKER_JMX_PORT: 9999
          BOOTSTRAP_BROKER_JMX_USER: admin
          BOOTSTRAP_BROKER_JMX_PASSWORD: password

          # Kerberos authentication arguments
          SASL_MECHANISM: GSSAPI
          SASL_GSSAPI_REALM: SOMECORP.COM
          SASL_GSSAPI_SERVICE_NAME: Kafka
          SASL_GSSAPI_USERNAME: kafka
          SASL_GSSAPI_KEY_TAB_PATH: /etc/newrelic-infra/kafka.keytab
          SASL_GSSAPI_KERBEROS_CONFIG_PATH: /etc/krb5.conf
          SASL_GSSAPI_DISABLE_FAST_NEGOTIATION: false
        interval: 15s
        labels:
          env: production
          role: kafka
        inventory_source: config/kafka
    ```

  </Collapser>

  <Collapser
    id="example6"
    title="ZOOKEEPER DISCOVERY TOPIC BUCKET"
  >
    This configuration collects Metrics splitting topic collection between 3 different instances:

    ```
    integrations:
      - name: nri-kafka
        env:
          METRICS: true
          CLUSTER_NAME: testcluster1
          KAFKA_VERSION: "1.0.0"
          AUTODISCOVER_STRATEGY: zookeeper
          ZOOKEEPER_HOSTS: '[{"host": "localhost", "port": 2181}]'
          ZOOKEEPER_AUTH_SECRET: "username:password"
          ZOOKEEPER_PATH: "/kafka-root"
          DEFAULT_JMX_USER: username
          DEFAULT_JMX_PASSWORD: password
          TOPIC_MODE: regex
          TOPIC_REGEX: 'topic\d+'
          TOPIC_BUCKET: '1/3'
        interval: 15s
        labels:
          env: production
          role: kafka
        inventory_source: config/kafka
      - name: nri-kafka
        env:
          METRICS: true
          CLUSTER_NAME: testcluster1
          KAFKA_VERSION: "1.0.0"
          AUTODISCOVER_STRATEGY: zookeeper
          ZOOKEEPER_HOSTS: '[{"host": "localhost", "port": 2181}]'
          ZOOKEEPER_AUTH_SECRET: "username:password"
          ZOOKEEPER_PATH: "/kafka-root"
          DEFAULT_JMX_USER: username
          DEFAULT_JMX_PASSWORD: password
          TOPIC_MODE: regex
          TOPIC_REGEX: 'topic\d+'
          TOPIC_BUCKET: '2/3'
        interval: 15s
        labels:
          env: production
          role: kafka
        inventory_source: config/kafka
      - name: nri-kafka
        env:
          METRICS: true
          CLUSTER_NAME: testcluster1
          KAFKA_VERSION: "1.0.0"
          AUTODISCOVER_STRATEGY: zookeeper
          ZOOKEEPER_HOSTS: '[{"host": "localhost", "port": 2181}]'
          ZOOKEEPER_AUTH_SECRET: "username:password"
          ZOOKEEPER_PATH: "/kafka-root"
          DEFAULT_JMX_USER: username
          DEFAULT_JMX_PASSWORD: password
          TOPIC_MODE: regex
          TOPIC_REGEX: 'topic\d+'
          TOPIC_BUCKET: '3/3'
        interval: 15s
        labels:
          env: production
          role: kafka
        inventory_source: config/kafka
    ```

  </Collapser>
  <Collapser
    id="example7"
    title="JAVA CONSUMER & PRODUCER"
  >
    This gives an example for collecting JMX metrics from Java consumers and producers:

    ```
    integrations:
      - name: nri-kafka
        env:
          METRICS: "true"
          CLUSTER_NAME: "testcluster3"
          PRODUCERS: '[{"name": "myProducer", "host": "localhost", "port": 24, "username": "me", "password": "secret"}]'
          CONSUMERS: '[{"name": "myConsumer", "host": "localhost", "port": 24, "username": "me", "password": "secret"}]'
          DEFAULT_JMX_HOST: "localhost"
          DEFAULT_JMX_PORT: "9999"
        interval: 15s
        labels:
          env: production
          role: kafka
        inventory_source: config/kafka
    ```

  </Collapser>
  <Collapser
    id="example8"
    title="CONSUMER OFFSET"
  >
    This configuration collects consumer offset Metrics and Inventory for the cluster:

    ```
    integrations:
      - name: nri-kafka
        env:
          CONSUMER_OFFSET: true
          CLUSTER_NAME: testcluster3
          AUTODISCOVER_STRATEGY: bootstrap
          BOOTSTRAP_BROKER_HOST: localhost
          BOOTSTRAP_BROKER_KAFKA_PORT: 9092
          BOOTSTRAP_BROKER_KAFKA_PROTOCOL: PLAINTEXT
          # A regex pattern that matches the consumer groups to collect metrics from
          CONSUMER_GROUP_REGEX: '.*'
        interval: 15s
        labels:
          env: production
          role: kafka
        inventory_source: config/kafka
    ```

  </Collapser>
</CollapserGroup>

## Find and use data [#find-and-use]

Data from this service is reported to an [integration dashboard](/docs/integrations/new-relic-integrations/getting-started/infrastructure-integration-dashboards-charts).

Kafka data is attached to the following [event types](/docs/using-new-relic/data/understand-data/new-relic-data-types#events-new-relic):

- [`KafkaBrokerSample`](#broker-sample)
- [`KafkaTopicSample`](#topic-sample)
- [`KafkaProducerSample`](#producer-sample)
- [`KafkaConsumerSample`](#consumer-sample)
- [`KafkaOffsetSample`](#offset-sample)

You can [query this data](/docs/using-new-relic/data/understand-data/query-new-relic-data) for troubleshooting purposes or to create charts and dashboards.

For more on how to find and use your data, see [Understand integration data](/docs/infrastructure/integrations/find-use-infrastructure-integration-data).



## Check the source code [#source-code]

