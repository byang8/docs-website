---
title: Kafka's integration configuration
tags:
  - Integrations
  - On-host integrations
  - On-host integrations list
  - Advanced configuration
metaDescription: Advanced configuration New Relic's Kafka integration.
---

This integration is open source software. That means you can [browse its source code](https://github.com/newrelic/nri-kafka 'Link opens in a new window.') and send improvements or create your own fork and build it.

## Labels and custom Attributes [#labels]

Environment variables can be used to control config settings, such as your license key, and are then passed through to the Infrastructure agent. For instructions on how to use this feature, see [Configure the Infrastructure agent](/docs/infrastructure/new-relic-infrastructure/configuration/configure-infrastructure-agent#passthrough).

You can further decorate your metrics using labels. Labels allow you to add key/value pairs attributes to your metrics which you can then use to query, filter or group your metrics on.<br/>
Our default sample config file includes examples of labels but, as they are not mandatory, you can remove, modify or add new ones of your choice.

```yaml
  labels:
    env: production
    role: kafka
```

For more about the general structure of on-host integration configuration, see the [configuration](/docs/integrations/integrations-sdk/file-specifications/host-integration-configuration-overview).

## Inventory data [#inventory]

The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the [Inventory UI page](/docs/infrastructure/new-relic-infrastructure/infrastructure-ui-pages/infrastructure-inventory-page-search-your-entire-infrastructure) under the `config/kafka` source.

## Configure KafkaBrokerSample and KafkaTopicSample collection [#broker-collection]

The Kafka integration collects both Metrics and Inventory information. Check the **Applies To** column below to see the settings available to each collection:

<table>
  <thead>
  <tr>
    <th style={{ width: "150px" }}>
      Setting
    </th>
    <th>
      Description
    </th>
    <th>
      Default
    </th>
    <th>
      Applies To
    </th>
  </tr>
  </thead>
  <tbody>
  <tr>
    <td>
      **CLUSTER_NAME**
    </td>
    <td>
      user-defined name to uniquely identify the cluster being monitored. **Required**.
    </td>
    <td>
      N/A
    </td>
    <td style={{ "text-align": "center" }}>
      M/I
    </td>
  </tr>

  <tr>
    <td>
      **KAFKA_VERSION**
    </td>
    <td>
      The version of the Kafka broker you're connecting to, used for setting optimum API versions. It must match -or be lower than- the version from the broker.

      Versions older than 1.0.0 may be missing some features.

      **Note that if the broker binary name is kafka_2.12-2.7.0 the Kafka api version to be used is 2.7.0, the preceding 2.12 is the Scala language version**.
    </td>
    <td>
      1.0.0
    </td>
    <td style={{ "text-align": "center" }}>
      M/I
    </td>

  </tr>

{' '}

<tr>
  <td>**AUTODISCOVER_STRATEGY**</td>
  <td>
    the method of discovering brokers. Options are `zookeeper` or `bootstrap`.
  </td>
  <td>zookeeper</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**METRICS**</td>
  <td>Set to `true` to enable Metrics only collection.</td>
  <td>false</td>
  <td style={{ 'text-align': 'center' }}></td>
</tr>

{' '}

<tr>
  <td>**INVENTORY**</td>
  <td>Set to `true` to enable Inventory only collection.</td>
  <td>false</td>
  <td style={{ 'text-align': 'center' }}></td>
</tr>

  </tbody>
</table>

## Zookeeper autodiscovery arguments [#zookeeper-discovery]

These are only relevant when the `autodiscover_strategy` option is set to `zookeeper`.

<table>
  <thead>
  <tr>
    <th style={{ width: "150px" }}>
      Setting
    </th>
    <th>
      Description
    </th>
    <th>
      Default
    </th>
    <th>
      Applies To
    </th>
  </tr>
  </thead>
  <tbody>
  <tr>
    <td>
      **ZOOKEEPER_HOSTS**
    </td>
    <td>
      The list of Apache ZooKeeper hosts (in JSON format) that need to be connected.

      ** If `CONSUMER_OFFSET` is set to `false` `KafkaBrokerSamples` and `KafkaTopicSamples` will be collected.**
    </td>
    <td>
      []
    </td>
    <td style={{ "text-align": "center" }}>
      M/I
    </td>

  </tr>

{' '}

<tr>
  <td>**ZOOKEEPER_AUTH_SCHEME**</td>
  <td>
    The ZooKeeper authentication scheme that is used to connect. Currently, the
    only supported value is `digest`. If omitted, no authentication is used.
  </td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**ZOOKEEPER_AUTH_SECRET**</td>
  <td>
    The ZooKeeper authentication secret that is used to connect. Should be of
    the form `username:password`. Only required if `zookeeper_auth_scheme` is
    specified.
  </td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**ZOOKEEPER_PATH**</td>
  <td>
    The Zookeeper node under which the Kafka configuration resides. Defaults to
    `/`.
  </td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**PREFERRED_LISTENER**</td>
  <td>
    Use a specific listener to connect to a broker. If unset, the first listener
    that passes a successful test connection is used. Supported values are
    `PLAINTEXT`, `SASL_PLAINTEXT`, `SSL`, and `SASL_SSL`. Note: The `SASL_*`
    protocols only support Kerberos (GSSAPI) authentication.
  </td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

  </tbody>
</table>

## Bootstrap broker discovery arguments [#bootstrap-discovery]

These are only relevant when the `autodiscover_strategy` option is set to`bootstrap`

<table>
  <thead>
  <tr>
    <th style={{ width: "150px" }}>
      Setting
    </th>
    <th>
      Description
    </th>
    <th>
      Default
    </th>
    <th>
      Applies To
    </th>
  </tr>
  </thead>
  <tbody>
  <tr>
    <td>
      **BOOTSTRAP_BROKER_HOST**
    </td>
    <td>
      The host for the bootstrap broker.

      ** If `CONSUMER_OFFSET` is set to `false` `KafkaBrokerSamples` and `KafkaTopicSamples` will be collected.**
    </td>
    <td>
      N/A
    </td>
    <td style={{ "text-align": "center" }}>
      M/I
    </td>

  </tr>

{' '}

<tr>
  <td>**BOOTSTRAP_BROKER_KAFKA_PORT**</td>
  <td>The Kafka port for the bootstrap broker.</td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

  <tr>
    <td>
      **BOOTSTRAP_BROKER_KAFKA_PROTOCOL**
    </td>
    <td>
      The protocol to use to connect to the bootstrap broker. Supported values are `PLAINTEXT`, `SASL_PLAINTEXT`, `SSL`, and `SASL_SSL`.

      **Note the `SASL_*` protocols only support Kerberos (GSSAPI) authentication.**
    </td>
    <td>
      PLAINTEXT
    </td>
    <td style={{ "text-align": "center" }}>
      M/I
    </td>

  </tr>

  <tr>
    <td>
      **BOOTSTRAP_BROKER_JMX_PORT**
    </td>
    <td>
      The JMX port to use for collection on each broker in the cluster.

      **Note that all discovered brokers should have JMX active on this port**
    </td>
    <td>
      N/A
    </td>
    <td style={{ "text-align": "center" }}>
      M/I
    </td>

  </tr>

{' '}

<tr>
  <td>**BOOTSTRAP_BROKER_JMX_USER**</td>
  <td>The JMX user to use for collection on each broker in the cluster.</td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**BOOTSTRAP_BROKER_JMX_PASSWORD**</td>
  <td>The JMX password to use for collection on each broker in the cluster.</td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

  </tbody>
</table>

## JMX options [#jmx]

These options apply to all JMX connections on the instance.

<table>
  <thead>
  <tr>
    <th style={{ width: "150px" }}>
      Setting
    </th>
    <th>
      Description
    </th>
    <th>
      Default
    </th>
    <th>
      Applies To
    </th>
  </tr>
  </thead>
  <tbody>
  <tr>
    <td>
      **KEY_STORE**
    </td>
    <td>
      The filepath of the keystore containing the JMX client's SSL certificate.
    </td>
    <td>
      N/A
    </td>
    <td style={{ "text-align": "center" }}>
      M/I
    </td>
  </tr>

{' '}

<tr>
  <td>**KEY_STORE_PASSWORD**</td>
  <td>The password for the JMX SSL key store.</td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**TRUST_STORE**</td>
  <td>
    The filepath of the trust keystore containing the JMX server's SSL
    certificate.
  </td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**TRUST_STORE_PASSWORD**</td>
  <td>The password for the JMX trust store.</td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**DEFAULT_JMX_USER**</td>
  <td>
    The default user that is connecting to the JMX host to collect metrics. If
    the username field is omitted for a JMX host, this value will be used.
  </td>
  <td>admin</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**DEFAULT_JMX_PASSWORD**</td>
  <td>
    The default password to connect to the JMX host. If the password field is
    omitted for a JMX host, this value will be used.
  </td>
  <td>admin</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**TIMEOUT**</td>
  <td>The timeout for individual JMX queries in milliseconds.</td>
  <td>10000</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

  </tbody>
</table>

## Broker TLS connection options [#broker-tls]

You need these options if the broker protocol is `SSL` or `SASL_SSL`.

<table>
  <thead>
  <tr>
    <th style={{ width: "150px" }}>
      Setting
    </th>
    <th>
      Description
    </th>
    <th>
      Default
    </th>
    <th>
      Applies To
    </th>
  </tr>
  </thead>
  <tbody>
  <tr>
    <td>
      **TLS_CA_FILE**
    </td>
    <td>
      The certificate authority file for SSL and SASL_SSL listeners, in PEM format.
    </td>
    <td>
      N/A
    </td>
    <td style={{ "text-align": "center" }}>
      M/I
    </td>
  </tr>

{' '}

<tr>
  <td>**TLS_CERT_FILE**</td>
  <td>
    The client certificate file for SSL and SASL_SSL listeners, in PEM format.
  </td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**TLS_KEY_FILE**</td>
  <td>The client key file for SSL and SASL_SSL listeners, in PEM format.</td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**TLS_INSECURE_SKIP_VERIFY**</td>
  <td>Skip verifying the server's certificate chain and host name.</td>
  <td>false</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

  </tbody>
</table>

## Broker SASL and Kerberos connection options [#broker-sasl-kerberos]

You need these options if the broker protocol is `SASL_PLAINTEXT` or `SASL_SSL`.

<table>
  <thead>
  <tr>
    <th style={{ width: "150px" }}>
      Setting
    </th>
    <th>
      Description
    </th>
    <th>
      Default
    </th>
    <th>
      Applies To
    </th>
  </tr>
  </thead>
  <tbody>
  <tr>
    <td>
      **SASL_MECHANISM**
    </td>
    <td>
      The type of SASL authentication to use. Supported options are `SCRAM-SHA-512`, `SCRAM-SHA-256`, `PLAIN`, and `GSSAPI`.
    </td>
    <td>
      N/A
    </td>
    <td style={{ "text-align": "center" }}>
      M/I
    </td>
  </tr>

{' '}

<tr>
  <td>**SASL_USERNAME**</td>
  <td>SASL username required with the PLAIN and SCRAM mechanisms.</td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**SASL_PASSWORD**</td>
  <td>SASL password required with the PLAIN and SCRAM mechanisms.</td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**SASL_GSSAPI_REALM**</td>
  <td>Kerberos realm required with the GSSAPI mechanism.</td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**SASL_GSSAPI_SERVICE_NAME**</td>
  <td>Kerberos service name required with the GSSAPI mechanism.</td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**SASL_GSSAPI_USERNAME**</td>
  <td>Kerberos username required with the GSSAPI mechanism.</td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**SASL_GSSAPI_KEY_TAB_PATH**</td>
  <td>Kerberos key tab path required with the GSSAPI mechanism.</td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**SASL_GSSAPI_KERBEROS_CONFIG_PATH**</td>
  <td>Kerberos config path required with the GSSAPI mechanism.</td>
  <td>/etc/krb5.conf</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**SASL_GSSAPI_DISABLE_FAST_NEGOTIATION**</td>
  <td>Disable FAST negotiation.</td>
  <td>false</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

  </tbody>
</table>

## Broker Collection filtering [#broker-filteri]

<table>
  <thead>
  <tr>
    <th style={{ width: "150px" }}>
      Setting
    </th>
    <th>
      Description
    </th>
    <th>
      Default
    </th>
    <th>
      Applies To
    </th>
  </tr>
  </thead>
  <tbody>

  <tr>
    <td>
      **LOCAL_ONLY_COLLECTION**
    </td>
    <td>
      Collect only the metrics related to the configured bootstrap broker. Only used if `autodiscover_strategy` is `bootstrap`.

      **Environments that use discovery (such as Kubernetes) must be set to true because othwerwise brokers will be discovered twice: By the integration, and by the discovery mechanism, leading to duplicate data.**

      **Note that activating this flag will skip KafkaTopicSample collection**
    </td>
    <td>
      false
    </td>
    <td style={{ "text-align": "center" }}>
      M/I
    </td>

  </tr>

{' '}

<tr>
  <td>**TOPIC_MODE**</td>
  <td>
    Determines how many topics we collect. Options are `all`, `none`, `list`, or
    `regex`.
  </td>
  <td>none</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**TOPIC_LIST**</td>
  <td>
    JSON array of topic names to monitor. Only in effect if `topic_mode` is set
    to `list`.
  </td>
  <td>[]</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**TOPIC_REGEX**</td>
  <td>
    Regex pattern that matches the topic names to monitor. Only in effect if
    `topic_mode` is set to `regex`.
  </td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

  <tr>
    <td>
      **TOPIC_BUCKET**
    </td>
    <td>
      Used to split topic collection across multiple instances. Should be of the form `<bucket number>/<number of buckets>`.
    </td>
    <td>
      1/1
    </td>
    <td style={{ "text-align": "center" }}>
      M/I
    </td>
  </tr>

  <tr>
    <td>
      **COLLECT_TOPIC_SIZE**
    </td>
    <td>
      Collect the metric Topic size. Options are `true` or `false`, defaults to `false`.

      **This is a resource-intensive metric to collect, especially against many topics.**
    </td>
    <td>
      false
    </td>
    <td style={{ "text-align": "center" }}>
      M/I
    </td>

  </tr>

  <tr>
    <td>
      **COLLECT_TOPIC_OFFSET**
    </td>
    <td>
      Collect the metric Topic offset. Options are `true` or `false`, defaults to `false`.

      **This is a resource-intensive metric to collect, especially against many topics.**
    </td>
    <td>
      false
    </td>
    <td style={{ "text-align": "center" }}>
      M/I
    </td>

  </tr>

  </tbody>
</table>

## Configure KafkaConsumerSample and KafkaProducerSample collection [#KafkaConsumerSample-collection]

The Kafka integration collects both Metrics(<strong>M</strong>) and Inventory(<strong>I</strong>) information. Check the **Applies To** column below to find which settings can be used for each specific collection:

<table>
  <thead>
  <tr>
    <th style={{ width: "150px" }}>
      Setting
    </th>
    <th>
      Description
    </th>
    <th>
      Default
    </th>
    <th>
      Applies To
    </th>
  </tr>
  </thead>
  <tbody>
  <tr>
    <td>
      **CLUSTER_NAME**
    </td>
    <td>
      user-defined name to uniquely identify the cluster being monitored. **Required**.
    </td>
    <td>
      N/A
    </td>
    <td style={{ "text-align": "center" }}>
      M/I
    </td>
  </tr>

  <tr>
    <td>
      **PRODUCERS**
    </td>
    <td>
      Producers to collect. For each provider a `name`, `hostname`, `port`, `username`, and `password` can be provided in JSON form. `name` is the producer’s name as it appears in Kafka. `hostname`, `port`, `username`, and `password` are the optional JMX settings and use the default if unspecified.
      Required to produce KafkaProducerSample.

      **Example: ```[{"name": "myProducer", "host": "localhost", "port": 24, "username": "me", "password": "secret"}]``` **
    </td>
    <td>
      []
    </td>
    <td style={{ "text-align": "center" }}>
      M/I
    </td>

  </tr>

  <tr>
    <td>
      **CONSUMERS**
    </td>
    <td>
      Consumers to collect. For each consumer a `name`, `hostname`, `port`, `username`, and `password` can be specified in JSON form. `name` is the consumer’s name as it appears in Kafka. `hostname`, `port`, `username`, and `password` are the optional JMX settings and use the default if unspecified.
      Required to produce KafkaConsumerSample.

      **Example: ```[{"name": "myConsumer", "host": "localhost", "port": 24, "username": "me", "password": "secret"}]``` **
    </td>
    <td>
      []
    </td>
    <td style={{ "text-align": "center" }}>
      M/I
    </td>

  </tr>

{' '}

<tr>
  <td>**DEFAULT_JMX_HOST**</td>
  <td>
    The default host to collect JMX metrics. If the host field is omitted from a
    producer or consumer configuration, this value will be used.
  </td>
  <td>localhost</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**DEFAULT_JMX_PORT**</td>
  <td>
    The default port to collect JMX metrics. If the port field is omitted from a
    producer or consumer configuration, this value will be used.
  </td>
  <td>9999</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**DEFAULT_JMX_USER**</td>
  <td>
    The default user that is connecting to the JMX host to collect metrics. If
    the username field is omitted from a producer or consumer configuration,
    this value will be used.
  </td>
  <td>admin</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**DEFAULT_JMX_PASSWORD**</td>
  <td>
    The default password to connect to the JMX host. If the password field is
    omitted from a producer or consumer configuration, this value will be used.
  </td>
  <td>admin</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**METRICS**</td>
  <td>Set to `true` to enable Metrics only collection.</td>
  <td>false</td>
  <td style={{ 'text-align': 'center' }}></td>
</tr>

{' '}

<tr>
  <td>**INVENTORY**</td>
  <td>Set to `true` to enable Inventory only collection.</td>
  <td>false</td>
  <td style={{ 'text-align': 'center' }}></td>
</tr>

  </tbody>
</table>

#### JMX SSL and timeout options

These options apply to all JMX connections on the instance.

<table>
  <thead>
  <tr>
    <th style={{ width: "150px" }}>
      Setting
    </th>
    <th>
      Description
    </th>
    <th>
      Default
    </th>
    <th>
      Applies To
    </th>
  </tr>
  </thead>
  <tbody>
    <tr>
      <td>
        **KEY_STORE**
      </td>
      <td>
        The filepath of the keystore containing the JMX client's SSL certificate.
      </td>
      <td>
        N/A
      </td>
      <td style={{ "text-align": "center" }}>
        M/I
      </td>
    </tr>

{' '}

    <tr>
      <td>**KEY_STORE_PASSWORD**</td>
      <td>The password for the JMX SSL key store.</td>
      <td>N/A</td>
      <td style={{ 'text-align': 'center' }}>M/I</td>
    </tr>

{' '}

    <tr>
      <td>**TRUST_STORE**</td>
      <td>
        The filepath of the trust keystore containing the JMX server's SSL
        certificate.
      </td>
      <td>N/A</td>
      <td style={{ 'text-align': 'center' }}>M/I</td>
    </tr>

{' '}

    <tr>
      <td>**TRUST_STORE_PASSWORD**</td>
      <td>The password for the JMX trust store.</td>
      <td>N/A</td>
      <td style={{ 'text-align': 'center' }}>M/I</td>
    </tr>

{' '}

    <tr>
      <td>**TIMEOUT**</td>
      <td>The timeout for individual JMX queries in milliseconds.</td>
      <td>10000</td>
      <td style={{ 'text-align': 'center' }}>M/I</td>
    </tr>
  </tbody>
</table>

## Configure KafkaOffsetSample collection [#KafkaOffsetSample-collection]

The Kafka integration collects both Metrics and Inventory information. Check the **Applies To** column below to find which settings can be used for each specific collection:

<table>
  <thead>
  <tr>
    <th style={{ width: "150px" }}>
      Setting
    </th>
    <th>
      Description
    </th>
    <th>
      Default
    </th>
    <th>
      Applies To
    </th>
  </tr>
  </thead>
  <tbody>
  <tr>
    <td>
      **CLUSTER_NAME**
    </td>
    <td>
      user-defined name to uniquely identify the cluster being monitored. **Required**.
    </td>
    <td>
      N/A
    </td>
    <td style={{ "text-align": "center" }}>
      M/I
    </td>
  </tr>

  <tr>
    <td>
      **KAFKA_VERSION**
    </td>
    <td>
      The version of the Kafka broker you're connecting to, used for setting optimum API versions. It must match -or be lower than- the version from the broker.

      Versions older than 1.0.0 may be missing some features.

      **Note that if the broker binary name is kafka_2.12-2.7.0 the Kafka api version to be used is 2.7.0, the preceding 2.12 is the Scala language version**.
    </td>
    <td>
      1.0.0
    </td>
    <td style={{ "text-align": "center" }}>
      M/I
    </td>

  </tr>

{' '}

<tr>
  <td>**AUTODISCOVER_STRATEGY**</td>
  <td>
    the method of discovering brokers. Options are `zookeeper` or `bootstrap`.
  </td>
  <td>zookeeper</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

  <tr>
    <td>
      **CONSUMER_OFFSET**
    </td>
    <td>
      Populate consumer offset data in KafkaOffsetSample if set to true.

      **Note that this option will skip Broker/Consumer/Producer collection and only collect KafkaOffsetSample**
    </td>
    <td>
      false
    </td>
    <td style={{ "text-align": "center" }}>
      M/I
    </td>

  </tr>

  <tr>
    <td>
      **CONSUMER_GROUP_REGEX**
    </td>
    <td>
      regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups.

      Note: `consumer_groups` has been deprecated, use this argument instead. This option must be set when CONSUMER_OFFSET is true.
    </td>
    <td>
      N/A
    </td>
    <td style={{ "text-align": "center" }}>
      M/I
    </td>

  </tr>

{' '}

<tr>
  <td>**METRICS**</td>
  <td>Set to `true` to enable Metrics only collection.</td>
  <td>false</td>
  <td style={{ 'text-align': 'center' }}></td>
</tr>

{' '}

<tr>
  <td>**INVENTORY**</td>
  <td>Set to `true` to enable Inventory only collection.</td>
  <td>false</td>
  <td style={{ 'text-align': 'center' }}></td>
</tr>

  </tbody>
</table>

## Zookeeper autodiscovery arguments [#zookeeper-autodiscovery]

This is only relevant when the `autodiscover_strategy` option is set to `zookeeper`.

<table>
  <thead>
  <tr>
    <th style={{ width: "150px" }}>
      Setting
    </th>
    <th>
      Description
    </th>
    <th>
      Default
    </th>
    <th>
      Applies To
    </th>
  </tr>
  </thead>
  <tbody>
  <tr>
    <td>
      **ZOOKEEPER_HOSTS**
    </td>
    <td>
      The list of Apache ZooKeeper hosts (in JSON format) that need to be connected.

      ** If `CONSUMER_OFFSET` is set to `false` `KafkaBrokerSamples` and `KafkaTopicSamples` will be collected.**
    </td>
    <td>
      []
    </td>
    <td style={{ "text-align": "center" }}>
      M/I
    </td>

  </tr>

{' '}

<tr>
  <td>**ZOOKEEPER_AUTH_SCHEME**</td>
  <td>
    The ZooKeeper authentication scheme that is used to connect. Currently, the
    only supported value is `digest`. If omitted, no authentication is used.
  </td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**ZOOKEEPER_AUTH_SECRET**</td>
  <td>
    The ZooKeeper authentication secret that is used to connect. Should be of
    the form `username:password`. Only required if `zookeeper_auth_scheme` is
    specified.
  </td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**ZOOKEEPER_PATH**</td>
  <td>
    The Zookeeper node under which the Kafka configuration resides. Defaults to
    `/`.
  </td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**PREFERRED_LISTENER**</td>
  <td>
    Use a specific listener to connect to a broker. If unset, the first listener
    that passes a successful test connection is used. Supported values are
    `PLAINTEXT`, `SASL_PLAINTEXT`, `SSL`, and `SASL_SSL`. Note: The `SASL_*`
    protocols only support Kerberos (GSSAPI) authentication.
  </td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

  </tbody>
</table>

## Bootstrap broker discovery arguments [#bootstrap-broker]
 
This is only relevant when the `autodiscover_strategy` option is set to `bootstrap`.

<table>
  <thead>
  <tr>
    <th style={{ width: "150px" }}>
      Setting
    </th>
    <th>
      Description
    </th>
    <th>
      Default
    </th>
    <th>
      Applies To
    </th>
  </tr>
  </thead>
  <tbody>
  <tr>
    <td>
      **BOOTSTRAP_BROKER_HOST**
    </td>
    <td>
      The host for the bootstrap broker.

      ** If `CONSUMER_OFFSET` is set to `false` `KafkaBrokerSamples` and `KafkaTopicSamples` will be collected.**
    </td>
    <td>
      N/A
    </td>
    <td style={{ "text-align": "center" }}>
      M/I
    </td>

  </tr>

{' '}

<tr>
  <td>**BOOTSTRAP_BROKER_KAFKA_PORT**</td>
  <td>The Kafka port for the bootstrap broker.</td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

  <tr>
    <td>
      **BOOTSTRAP_BROKER_KAFKA_PROTOCOL**
    </td>
    <td>
      The protocol to use to connect to the bootstrap broker. Supported values are `PLAINTEXT`, `SASL_PLAINTEXT`, `SSL`, and `SASL_SSL`.

      **Note the `SASL_*` protocols only support Kerberos (GSSAPI) authentication.**
    </td>
    <td>
      PLAINTEXT
    </td>
    <td style={{ "text-align": "center" }}>
      M/I
    </td>

  </tr>

  <tr>
    <td>
      **BOOTSTRAP_BROKER_JMX_PORT**
    </td>
    <td>
      The JMX port to use for collection on each broker in the cluster.

      **Note that all discovered brokers should have JMX active on this port**
    </td>
    <td>
      N/A
    </td>
    <td style={{ "text-align": "center" }}>
      M/I
    </td>

  </tr>

{' '}

<tr>
  <td>**BOOTSTRAP_BROKER_JMX_USER**</td>
  <td>The JMX user to use for collection on each broker in the cluster.</td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**BOOTSTRAP_BROKER_JMX_PASSWORD**</td>
  <td>The JMX password to use for collection on each broker in the cluster.</td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

  </tbody>
</table>

## JMX SSL and timeout options [#jmx-ssl-timeout]

These apply to all JMX connections on an instance.

<table>
  <thead>
  <tr>
    <th style={{ width: "150px" }}>
      Setting
    </th>
    <th>
      Description
    </th>
    <th>
      Default
    </th>
    <th>
      Applies To
    </th>
  </tr>
  </thead>
  <tbody>
  <tr>
    <td>
      **KEY_STORE**
    </td>
    <td>
      The filepath of the keystore containing the JMX client's SSL certificate.
    </td>
    <td>
      N/A
    </td>
    <td style={{ "text-align": "center" }}>
      M/I
    </td>
  </tr>

{' '}

<tr>
  <td>**KEY_STORE_PASSWORD**</td>
  <td>The password for the JMX SSL key store.</td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**TRUST_STORE**</td>
  <td>
    The filepath of the trust keystore containing the JMX server's SSL
    certificate.
  </td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**TRUST_STORE_PASSWORD**</td>
  <td>The password for the JMX trust store.</td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**DEFAULT_JMX_USER**</td>
  <td>
    The default user that is connecting to the JMX host to collect metrics. If
    the username field is omitted for a JMX host, this value will be used.
  </td>
  <td>admin</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**DEFAULT_JMX_PASSWORD**</td>
  <td>
    The default password to connect to the JMX host. If the password field is
    omitted for a JMX host, this value will be used.
  </td>
  <td>admin</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**TIMEOUT**</td>
  <td>The timeout for individual JMX queries in milliseconds.</td>
  <td>10000</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

  </tbody>
</table>

## Broker TLS connection options [#broker-tls]

You only need this if the broker protocol is `SSL` or `SASL_SSL`.

<table>
  <thead>
  <tr>
    <th style={{ width: "150px" }}>
      Setting
    </th>
    <th>
      Description
    </th>
    <th>
      Default
    </th>
    <th>
      Applies To
    </th>
  </tr>
  </thead>
  <tbody>
  <tr>
    <td>
      **TLS_CA_FILE**
    </td>
    <td>
      The certificate authority file for SSL and SASL_SSL listeners, in PEM format.
    </td>
    <td>
      N/A
    </td>
    <td style={{ "text-align": "center" }}>
      M/I
    </td>
  </tr>

{' '}

<tr>
  <td>**TLS_CERT_FILE**</td>
  <td>
    The client certificate file for SSL and SASL_SSL listeners, in PEM format.
  </td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**TLS_KEY_FILE**</td>
  <td>The client key file for SSL and SASL_SSL listeners, in PEM format.</td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**TLS_INSECURE_SKIP_VERIFY**</td>
  <td>Skip verifying the server's certificate chain and host name.</td>
  <td>false</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

  </tbody>
</table>

## Broker SASL and Kerberos connection options [#broker-sasl-kerberos]

You only need these if the broker protocol is `SASL_PLAINTEXT` or `SASL_SSL`.

<table>
  <thead>
  <tr>
    <th style={{ width: "150px" }}>
      Setting
    </th>
    <th>
      Description
    </th>
    <th>
      Default
    </th>
    <th>
      Applies To
    </th>
  </tr>
  </thead>
  <tbody>
  <tr>
    <td>
      **SASL_MECHANISM**
    </td>
    <td>
      The type of SASL authentication to use. Supported options are `SCRAM-SHA-512`, `SCRAM-SHA-256`, `PLAIN`, and `GSSAPI`.
    </td>
    <td>
      N/A
    </td>
    <td style={{ "text-align": "center" }}>
      M/I
    </td>
  </tr>

{' '}

<tr>
  <td>**SASL_USERNAME**</td>
  <td>SASL username required with the PLAIN and SCRAM mechanisms.</td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**SASL_PASSWORD**</td>
  <td>SASL password required with the PLAIN and SCRAM mechanisms.</td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**SASL_GSSAPI_REALM**</td>
  <td>Kerberos realm required with the GSSAPI mechanism.</td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**SASL_GSSAPI_SERVICE_NAME**</td>
  <td>Kerberos service name required with the GSSAPI mechanism.</td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**SASL_GSSAPI_USERNAME**</td>
  <td>Kerberos username required with the GSSAPI mechanism.</td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**SASL_GSSAPI_KEY_TAB_PATH**</td>
  <td>Kerberos key tab path required with the GSSAPI mechanism.</td>
  <td>N/A</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**SASL_GSSAPI_KERBEROS_CONFIG_PATH**</td>
  <td>Kerberos config path required with the GSSAPI mechanism.</td>
  <td>/etc/krb5.conf</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

{' '}

<tr>
  <td>**SASL_GSSAPI_DISABLE_FAST_NEGOTIATION**</td>
  <td>Disable FAST negotiation.</td>
  <td>false</td>
  <td style={{ 'text-align': 'center' }}>M/I</td>
</tr>

  </tbody>
</table>

## Metric data [#metrics]

The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as `broker.` or `consumer.`.

### KafkaBrokerSample event [#broker-sample]

<table>
  <thead>
    <tr>
      <th style={{ width: "350px" }}>
        Metric
      </th>

      <th>
        Description
      </th>
    </tr>

  </thead>

  <tbody>
    <tr>
      <td>
        `broker.bytesWrittenToTopicPerSecond`
      </td>

      <td>
        Number of bytes written to a topic by the broker per second.
      </td>
    </tr>

    <tr>
      <td>
        `broker.IOInPerSecond`
      </td>

      <td>
        Network IO into brokers in the cluster in bytes per second.
      </td>
    </tr>

    <tr>
      <td>
        `broker.IOOutPerSecond`
      </td>

      <td>
        Network IO out of brokers in the cluster in bytes per second.
      </td>
    </tr>

    <tr>
      <td>
        `broker.logFlushPerSecond`
      </td>

      <td>
        Log flush rate.
      </td>
    </tr>

    <tr>
      <td>
        `broker.messagesInPerSecond`
      </td>

      <td>
        Incoming messages per second.
      </td>
    </tr>

    <tr>
      <td>
        `follower.requestExpirationPerSecond`
      </td>

      <td>
        Rate of request expiration on followers in evictions per second.
      </td>
    </tr>

    <tr>
      <td>
        `net.bytesRejectedPerSecond`
      </td>

      <td>
        Rejected bytes per second.
      </td>
    </tr>

    <tr>
      <td>
        `replication.isrExpandsPerSecond`
      </td>

      <td>
        Rate of replicas joining the ISR pool.
      </td>
    </tr>

    <tr>
      <td>
        `replication.isrShrinksPerSecond`
      </td>

      <td>
        Rate of replicas leaving the ISR pool.
      </td>
    </tr>

    <tr>
      <td>
        `replication.leaderElectionPerSecond`
      </td>

      <td>
        Leader election rate.
      </td>
    </tr>

    <tr>
      <td>
        `replication.uncleanLeaderElectionPerSecond`
      </td>

      <td>
        Unclean leader election rate.
      </td>
    </tr>

    <tr>
      <td>
        `replication.unreplicatedPartitions`
      </td>

      <td>
        Number of unreplicated partitions.
      </td>
    </tr>

    <tr>
      <td>
        `request.avgTimeFetch`
      </td>

      <td>
        Average time per fetch request in milliseconds.
      </td>
    </tr>

    <tr>
      <td>
        `request.avgTimeMetadata`
      </td>

      <td>
        Average time for metadata request in milliseconds.
      </td>
    </tr>

    <tr>
      <td>
        `request.avgTimeMetadata99Percentile`
      </td>

      <td>
        Time for metadata requests for 99th percentile in milliseconds.
      </td>
    </tr>

    <tr>
      <td>
        `request.avgTimeOffset`
      </td>

      <td>
        Average time for an offset request in milliseconds.
      </td>
    </tr>

    <tr>
      <td>
        `request.avgTimeOffset99Percentile`
      </td>

      <td>
        Time for offset requests for 99th percentile in milliseconds.
      </td>
    </tr>

    <tr>
      <td>
        `request.avgTimeProduceRequest`
      </td>

      <td>
        Average time for a produce request in milliseconds.
      </td>
    </tr>

    <tr>
      <td>
        `request.avgTimeUpdateMetadata`
      </td>

      <td>
        Average time for a request to update metadata in milliseconds.
      </td>
    </tr>

    <tr>
      <td>
        `request.avgTimeUpdateMetadata99Percentile`
      </td>

      <td>
        Time for update metadata requests for 99th percentile in milliseconds.
      </td>
    </tr>

    <tr>
      <td>
        `request.clientFetchesFailedPerSecond`
      </td>

      <td>
        Client fetch request failures per second.
      </td>
    </tr>

    <tr>
      <td>
        `request.fetchTime99Percentile`
      </td>

      <td>
        Time for fetch requests for 99th percentile in milliseconds.
      </td>
    </tr>

    <tr>
      <td>
        `request.handlerIdle`
      </td>

      <td>
        Average fraction of time the request handler threads are idle.
      </td>
    </tr>

    <tr>
      <td>
        `request.produceRequestsFailedPerSecond`
      </td>

      <td>
        Failed produce requests per second.
      </td>
    </tr>

    <tr>
      <td>
        `request.produceTime99Percentile`
      </td>

      <td>
        Time for produce requests for 99th percentile.
      </td>
    </tr>

    <tr>
      <td>
        `topic.diskSize`
      </td>

      <td>
        In disk Topic size. Only present if COLLECT_TOPIC_SIZE is enabled.
      </td>
    </tr>

    <tr>
      <td>
        `topic.offset`
      </td>

      <td>
        Topic offset. Only present if COLLECT_TOPIC_OFFSET is enabled.
      </td>
    </tr>

  </tbody>
</table>

### KafkaConsumerSample event [#consumer-sample]

<table>
  <thead>
    <tr>
      <th style={{ width: "350px" }}>
        Metric
      </th>

      <th>
        Description
      </th>
    </tr>

  </thead>

  <tbody>
    <tr>
      <td>
        `consumer.avgFetchSizeInBytes`
      </td>

      <td>
        Average number of bytes fetched per request for a specific topic.
      </td>
    </tr>

    <tr>
      <td>
        `consumer.avgRecordConsumedPerTopic`
      </td>

      <td>
        Average number of records in each request for a specific topic.
      </td>
    </tr>

    <tr>
      <td>
        `consumer.avgRecordConsumedPerTopicPerSecond`
      </td>

      <td>
        Average number of records consumed per second for a specific topic in records per second.
      </td>
    </tr>

    <tr>
      <td>
        `consumer.bytesInPerSecond`
      </td>

      <td>
        Consumer bytes per second.
      </td>
    </tr>

    <tr>
      <td>
        `consumer.fetchPerSecond`
      </td>

      <td>
        The minimum rate at which the consumer sends fetch requests to a broke in requests per second.
      </td>
    </tr>

    <tr>
      <td>
        `consumer.maxFetchSizeInBytes`
      </td>

      <td>
        Maximum number of bytes fetched per request for a specific topic.
      </td>
    </tr>

    <tr>
      <td>
        `consumer.maxLag`
      </td>

      <td>
        Maximum consumer lag.
      </td>
    </tr>

    <tr>
      <td>
        `consumer.messageConsumptionPerSecond`
      </td>

      <td>
        Rate of consumer message consumption in messages per second.
      </td>
    </tr>

    <tr>
      <td>
        `consumer.offsetKafkaCommitsPerSecond`
      </td>

      <td>
        Rate of offset commits to Kafka in commits per second.
      </td>
    </tr>

    <tr>
      <td>
        `consumer.offsetZooKeeperCommitsPerSecond`
      </td>

      <td>
        Rate of offset commits to ZooKeeper in writes per second.
      </td>
    </tr>

    <tr>
      <td>
        `consumer.requestsExpiredPerSecond`
      </td>

      <td>
        Rate of delayed consumer request expiration in evictions per second.
      </td>
    </tr>

  </tbody>
</table>

### KafkaProducerSample event [#producer-sample]

<table>
  <thead>
    <tr>
      <th style={{ width: "350px" }}>
        Metric
      </th>

      <th>
        Description
      </th>
    </tr>

  </thead>

  <tbody>
    <tr>
      <td>
        `producer.ageMetadataUsedInMilliseconds`
      </td>

      <td>
        Age in seconds of the current producer metadata being used.
      </td>
    </tr>

    <tr>
      <td>
        `producer.availableBufferInBytes`
      </td>

      <td>
        Total amount of buffer memory that is not being used in bytes.
      </td>
    </tr>

    <tr>
      <td>
        `producer.avgBytesSentPerRequestInBytes`
      </td>

      <td>
        Average number of bytes sent per partition per-request.
      </td>
    </tr>

    <tr>
      <td>
        `producer.avgCompressionRateRecordBatches`
      </td>

      <td>
        Average compression rate of record batches.
      </td>
    </tr>

    <tr>
      <td>
        `producer.avgRecordAccumulatorsInMilliseconds`
      </td>

      <td>
        Average time in ms record batches spent in the record accumulator.
      </td>
    </tr>

    <tr>
      <td>
        `producer.avgRecordSizeInBytes`
      </td>

      <td>
        Average record size in bytes.
      </td>
    </tr>

    <tr>
      <td>
        `producer.avgRecordsSentPerSecond`
      </td>

      <td>
        Average number of records sent per second.
      </td>
    </tr>

    <tr>
      <td>
        `producer.avgRecordsSentPerTopicPerSecond`
      </td>

      <td>
        Average number of records sent per second for a topic.
      </td>
    </tr>

    <tr>
      <td>
        `producer.AvgRequestLatencyPerSecond`
      </td>

      <td>
        Producer average request latency.
      </td>
    </tr>

    <tr>
      <td>
        `producer.avgThrottleTime`
      </td>

      <td>
        Average time that a request was throttled by a broker in milliseconds.
      </td>
    </tr>

    <tr>
      <td>
        `producer.bufferMemoryAvailableInBytes`
      </td>

      <td>
        Maximum amount of buffer memory the client can use in bytes.
      </td>
    </tr>

    <tr>
      <td>
        `producer.bufferpoolWaitTime`
      </td>

      <td>
        Faction of time an appender waits for space allocation.
      </td>
    </tr>

    <tr>
      <td>
        `producer.bytesOutPerSecond`
      </td>

      <td>
        Producer bytes per second out.
      </td>
    </tr>

    <tr>
      <td>
        `producer.compressionRateRecordBatches`
      </td>

      <td>
        Average compression rate of record batches for a topic.
      </td>
    </tr>

    <tr>
      <td>
        `producer.iOWaitTime`
      </td>

      <td>
        Producer I/O wait time in milliseconds.
      </td>
    </tr>

    <tr>
      <td>
        `producer.maxBytesSentPerRequestInBytes`
      </td>

      <td>
        Max number of bytes sent per partition per-request.
      </td>
    </tr>

    <tr>
      <td>
        `producer.maxRecordSizeInBytes`
      </td>

      <td>
        Maximum record size in bytes.
      </td>
    </tr>

    <tr>
      <td>
        `producer.maxRequestLatencyInMilliseconds`
      </td>

      <td>
        Maximum request latency in milliseconds.
      </td>
    </tr>

    <tr>
      <td>
        `producer.maxThrottleTime`
      </td>

      <td>
        Maximum time a request was throttled by a broker in milliseconds.
      </td>
    </tr>

    <tr>
      <td>
        `producer.messageRatePerSecond`
      </td>

      <td>
        Producer messages per second.
      </td>
    </tr>

    <tr>
      <td>
        `producer.responsePerSecond`
      </td>

      <td>
        Number of producer responses per second.
      </td>
    </tr>

    <tr>
      <td>
        `producer.requestPerSecond`
      </td>

      <td>
        Number of producer requests per second.
      </td>
    </tr>

    <tr>
      <td>
        `producer.requestsWaitingResponse`
      </td>

      <td>
        Current number of in-flight requests awaiting a response.
      </td>
    </tr>

    <tr>
      <td>
        `producer.threadsWaiting`
      </td>

      <td>
        Number of user threads blocked waiting for buffer memory to enqueue their records.
      </td>
    </tr>

  </tbody>
</table>

### KafkaTopicSample event [#topic-sample]

<table>
  <thead>
    <tr>
      <th style={{ width: "350px" }}>
        Metric
      </th>

      <th>
        Description
      </th>
    </tr>

  </thead>

  <tbody>
    <tr>
      <td>
        `topic.diskSize`
      </td>

      <td>
        Current topic disk size per broker in bytes.
      </td>
    </tr>

    <tr>
      <td>
        `topic.partitionsWithNonPreferredLeader`
      </td>

      <td>
        Number of partitions per topic that are not being led by their preferred replica.
      </td>
    </tr>

    <tr>
      <td>
        `topic.respondMetaData`
      </td>

      <td>
        Number of topics responding to meta data requests.
      </td>
    </tr>

    <tr>
      <td>
        `topic.retentionSizeOrTime`
      </td>

      <td>
        Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time.
      </td>
    </tr>

    <tr>
      <td>
        `topic.underReplicatedPartitions`
      </td>

      <td>
        Number of partitions per topic that are under-replicated.
      </td>
    </tr>

  </tbody>
</table>

### KafkaOffsetSample event [#offset-sample]

<table>
  <thead>
    <tr>
      <th style={{ width: "350px" }}>
        Metric
      </th>

      <th>
        Description
      </th>
    </tr>

  </thead>

  <tbody>
    <tr>
      <td>
        `consumer.offset`
      </td>

      <td>
        The last consumed offset on a partition by the consumer group.
      </td>
    </tr>

    <tr>
      <td>
        `consumer.lag`
      </td>

      <td>
        The difference between a broker's high water mark and the consumer's offset (`consumer.hwm` - `consumer.offset`).
      </td>
    </tr>

    <tr>
      <td>
        `consumer.hwm`
      </td>

      <td>
        The offset of the last message written to a partition (high water mark).
      </td>
    </tr>

    <tr>
      <td>
        `consumer.totalLag`
      </td>

      <td>
        The sum of lags across partitions consumed by a consumer.
      </td>
    </tr>

    <tr>
      <td>
        `consumerGroup.totalLag`
      </td>

      <td>
        The sum of lags across all partitions consumed by a `consumerGroup`.
      </td>
    </tr>

    <tr>
      <td>
        `consumerGroup.maxLag`
      </td>

      <td>
        The maximum lag across all partitions consumed by a `consumerGroup`.
      </td>
    </tr>

  </tbody>
</table>

## Troubleshooting [#troubleshooting]

<CollapserGroup>
  <Collapser
    id="duplicate-info"
    title="Duplicate data being reported"
  >
    For agents monitoring producers and/or consumers, and that have `Topic mode` set to `All`:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the [configuration option](#config) `Collect topic size` is set to false.
  </Collapser>

{' '}

  <Collapser
    id="zookeeper-node-not-found"
    title="Integration is logging errors 'zk: node not found'"
  >
    Ensure that `zookeeper_path` is set correctly in the [configuration file](#config).
  </Collapser>

  <Collapser
    id="jmx-connection-errors"
    title="JMX connection errors"
  >
    The Kafka integration uses a JMX helper tool called `nrjmx` to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port.

    To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the `PORT`, `USERNAME`, and `PASSWORD` variables with the corresponding JMX settings for the brokers:

    ```shell
    echo "*:*" | nrjmx -hostname MY_HOSTNAME -port MY_PORT -v -username MY_USERNAME -password MY_PASSWORD
    ```

    The command should generate the output showing a long series of metrics without any errors.

  </Collapser>

  <Collapser
    id="kerberos-authentication"
    title="Kerberos authentication failing"
  >

    The integration might show an error like the following:

    ```shell
    KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database
    ```

    Check the keytab with kinit command. Replace the highlighted fields with your values:

    ```shell
    $ kinit -k -t <var>KEY_TAB_PATH</var> <var>USERNAME</var>
    ```

    If the username/keytab combination is correct, the command above should finish without printing any errors.

    Check the realm using klist command:

    ```shell
    $ klist |grep "Default principal:"
    ```

    You should see something like this:

    ```shell
    Default principal: johndoe@a_realm_name
    ```

    Check that the printed user name and realm match the `sasl_gssapi_realm` and `sasl_gssapi_username` parameters in the integration configuration.
  </Collapser>
</CollapserGroup>
